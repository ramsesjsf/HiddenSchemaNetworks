name: pcfg/schema_atomic3/
num_runs: 1
num_workers: 1
distributed: false
world_size: 1
gpus: !!python/tuple ['0']
seed: 1

model:
  module: hiddenschemanetworks.models.atomic_models
  name: RealSchemataAtomic2
  args:
    loss_type: VAE   # AE, VAE, VAE-MI
    n_symbols: 50
    mlp_prior_layers: #!!python/tuple [128, 128]  # must be list or null
    Erdos_edge_prob: 0.5   # 0.5, 0.1
    word_dropout: 0.0      # 0.0
    kl_threshold_rw: 0.1   # 0.5, 0.0
    kl_threshold_graph: 0.0
    onehot_symbols: true
    ################# GRAPH
    hard_graph_samples: false
    fully_connected_graph: false
    train_graph: true
    graph_generator:
      module: hiddenschemanetworks.models.blocks
      name: GraphGenerator
      args:
        symbols2hidden_layers: !!python/tuple [512, 512]  # must be list!
        nonlinearity: LeakyReLU
        normalization: false
        gamma_variables: false
        mean_field: false
        bernoulli_gamma_link: false
        n_communities: 128
        intercommunity_interaction: true
        diag_in_adj_matrix: true
        symbol_pair2link_function: true
        aggregated_kl: true      # if kl_graph is computed using the average posterior link prob
    ################# ENCODER
    encoder:
      module: hiddenschemanetworks.models.atomic_blocks
      name: EncoderAtomic3
      args:
        encoder_type: BERT  # ONLY BERT allowed
        rw_length: [5]       # MUST BE MULTIPLE OF 2
        custom_init: false
        object_from_prior: false
        pos_encoding: true
        n_layers_prior: 2

        hidden_size: 512
        num_hidden_layers: 6
        num_attention_heads: 8
        intermediate_size: 2048
        pretrained: False
    ################# DECODER
    decoder:
      module: hiddenschemanetworks.models.blocks_transformers
      name: DecoderSchema
      args:
        decoder_type: GPT2-PseudoSelfAttention     # GPT2-CrossAttention, GPT2-PseudoSelfAttention
        custom_init: false

        hidden_size: 512
        num_hidden_layers: 6
        num_attention_heads: 8
        intermediate_size: 2048
        pretrained: False

data_loader:
  module: hiddenschemanetworks.data.dataloaders
  name: DataLoaderPCFG
  args:
    batch_size: 128
    path_to_data: /raid/data/pcfg
    atomic_style: true
    ae_style: true
    train_on_object_only: [false]

optimizer:
  module: torch.optim
  name: Adam
  args:
    lr: 0.0001
    betas: !!python/tuple [0.9, 0.998]
  gradient_norm_clipping: 1

trainer:
  module: hiddenschemanetworks.trainer
  name: TrainerRealSchema
  args:
    bm_metric: NLL-Loss
    save_after_epoch: 20
    reconstruction_every: 500
    num_rec_sentences: 10
    num_samples: 0
    schedulers: !!python/tuple
      - module: hiddenschemanetworks.utils.param_scheduler
        name: WarmupScheduler
        label: lr_scheduler
        args:
          max_value: 2.0
          warmup_steps: 8000
      - module: hiddenschemanetworks.utils.param_scheduler
        name: PeriodicScheduler  # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
        label: beta_scheduler_kl_rws
        args:
          max_value: 0.1                  # for exp. sched.
          beta: 1.0                       # for const. sched.
          training_fraction_to_reach_max: 0.5  # for exp. sched.False
          validation_value: 1.0
      - module: hiddenschemanetworks.utils.param_scheduler
        name: ExponentialSchedulerGumbel
        label: temperature_scheduler_rws
        args:
          temp_init: 0.75
          min_temp: 0.75
          training_fraction_to_reach_min: 0.7
          validation_value: 1.0
      ################# GRAPH ##########################
      - module: hiddenschemanetworks.utils.param_scheduler
        name: PeriodicScheduler   # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
        label: beta_scheduler_kl_graph
        args:
          beta: 1.0                       # for const. sched.
          max_value: 10.0                  # for exp. sched.
          training_fraction_to_reach_max: 0.5   # for exp. sched.
          validation_value: 0.1
      - module: hiddenschemanetworks.utils.param_scheduler
        name: ExponentialSchedulerGumbel
        label: temperature_scheduler_graph
        args:
          temp_init: 0.75
          min_temp: 0.75
          training_fraction_to_reach_min: 0.7
          validation_value: 1.0
  epochs: 200
  save_dir: /rdata/results
  logging:
    tensorboard_dir: /rdata/results
    logging_dir: /rdata/results
    formatters:
      verbose: "%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s"
      simple: "%(levelname)s %(asctime)s %(message)s"